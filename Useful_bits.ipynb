{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code and brief explanation of different EMA methods, steps that I have used before or from EMA website\n",
    "# This notebook acts as a handy quick ref when I need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Refer for structure\n",
    "# Open exploration June 27 uncertaintystuff\n",
    "# IJssel_River_Project.ipynb\n",
    "# IJssel_River_Project_Open exploration_June 27.ipynb\n",
    "# Optimization Moro.ipynb\n",
    "\n",
    "# EMA modules: https://emaworkbench.readthedocs.io/en/latest/api_index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# sobol with zero policy, use excel to find 10 bad scenarios\n",
    "\n",
    "# Step 2\n",
    "# define these scenarios and do latin hypercube sampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZeroPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad scenario\n",
    "\n",
    "# this is not the worst scenario, other analyses should be done to find that, maybe optimization. But it is *a* bad scenario since we maximized bad stuff that could happen, except for floodwaveshap because I am not sure that maximizing that is actually the worst\n",
    "# A single scenario may not be the worst, so choose a small representative set\n",
    "\n",
    "# Sampling over levers\n",
    "\n",
    "# setup model, uncertainties, levers\n",
    "problem = get_SALib_problem(dike_model.uncertainties)\n",
    "\n",
    "bad = {'A.1_Bmax': 350, 'A.2_Bmax': 350, 'A.3_Bmax': 350, 'A.4_Bmax': 350, 'A.5_Bmax': 350,\n",
    "         'A.1_pfail': 0, 'A.2_pfail': 0, 'A.3_pfail': 0, 'A.4_pfail': 0, 'A.5_pfail': 0,\n",
    "         'A.1_Brate': 10, 'A.2_Brate': 10, 'A.3_Brate': 10, 'A.4_Brate': 10, 'A.5_Brate': 10,\n",
    "         'A.0_ID flood wave shape': 132, 'discount rate 0': 1.5, 'discount rate 1': 1.5, 'discount rate 2': 1.5,\n",
    "         }\n",
    "\n",
    "bad_scenario = {}\n",
    "\n",
    "\n",
    "for key in dike_model.uncertainties:\n",
    "        bad_scenario.update({key.name: bad[key.name]})\n",
    "\n",
    "scenario_bad = Scenario('bad', **bad_scenario)\n",
    "\n",
    "\n",
    "# Simulate using bad scenario\n",
    "# n_policy_sobol = 250 #16000 policies \n",
    "\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "       results_sobol = evaluator.perform_experiments(scenario_bad, policies = n_policy_sobol, levers_sampling=SOBOL)\n",
    "\n",
    "experiments_sobol, outcomes_sobol = results_sobol\n",
    "\n",
    "# save_results(results_sobol, 'SOBOL bad_scenario 16000 policies.tar.gz')\n",
    "# outcomes to data frame and then excel\n",
    "# outcomes_df = pd.DataFrame(outcomes_sobol)\n",
    "# experiments_df = pd.DataFrame(experiments_sobol)\n",
    "# df_results =pd.concat([outcomes_df, experiments_df], axis=1)\n",
    "# df_results.to_excel(\"results_sobol_bs_16000p.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step X\n",
    "# Choose larrge number of Policies (~1000?) and bad scenarios to evaluate\n",
    "# save these to gz and xl\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### EMA overview (open exp + optimizations/ search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Open Exp methods ##################\n",
    "# sampling based evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOBOL\n",
    "# can be chosen as sampling method directly in evaluator,either over uncertainties or levers\n",
    "# result dict can be analyzed wrt sobol indices dor sensitivity\n",
    "\n",
    "problem = get_SALib_problem(dike_model.uncertainties)\n",
    "\n",
    "y_deaths = outcomes_sobol['Expected Number of Deaths'] # define y here or directly inside the args. Might be useful to do it here to reduce my 2D outcome arrays\n",
    "y_damage = outcomes_sobol['Expected Annual Damage']\n",
    "#\n",
    "# Analyze wrt St and S1. \n",
    "\n",
    "problem = get_SALib_problem(model.uncertainties)\n",
    "Si = sobol.analyze(problem, outcomes['max_P'], calc_second_order=True, print_to_console=False)\n",
    "\n",
    "scores_filtered = {k:Si[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "Si_df = pd.DataFrame(scores_filtered, index=problem['names'])\n",
    "sobol_df.sort_values(by = 'ST', ascending=False) # sorting by St can be useful\n",
    "\n",
    "#  Visualize The error bars indicate the confidence intervals.\n",
    "sns.set_style('white')\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "indices = Si_df[['S1','ST']]\n",
    "err = Si_df[['S1_conf','ST_conf']]\n",
    "\n",
    "indices.plot.bar(yerr=err.values.T,ax=ax)\n",
    "fig.set_size_inches(8,6)\n",
    "fig.subplots_adjust(bottom=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can inspect any of the points on the trade off curve using the inspect method. As shown, we can show the results either in a table format or in a visual format.\n",
    "\n",
<<<<<<< HEAD
    "# Scenario discovery through PRIM vizualization\n",
=======
    "# Scenario discovery through PRIM viz\n",
>>>>>>> 494596a53c200ee8f66c578fe9701c496a9ec4dd
    "# Box\n",
    "# Choose Box/ scenario with best density-coverage tradeoff and then least params constrained, to inspect constribution to coverage and density from each contributing inputs, starting from the highest coverage contributor to the lowest. Delta density can be seen when each next contributor is added. This means that it is path dependent, impact (var1) impact (var1 & var2), impact of (var1, 2, 3, 4) up to number of variables constrained. This is why we want to look at the scenarios with as low variables constrained as possible.\n",
    "\n",
    "box1 = prim_alg.find_box()\n",
    "box1.show_tradeoff()\n",
    "plt.show()\n",
    "\n",
    "box1.inspect(43) \n",
    "box1.inspect(43, style='graph')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot\n",
    "# Choose multiple boxes of interest to compare\n",
    "box1.show_pairs_scatter(43)\n",
    "plt.show()\n",
    "\n",
    "# We have now found a first box that explains close to 80% of the cases of interest. Let’s see if we can find a second box that explains the remainder of the cases.\n",
    "box2 = prim_alg.find_box()\n",
    "\n",
    "# The logging will inform us in this case that no additional box can be found. The best coverage we can achieve is 0.35, which is well below the specified 0.8 threshold\n",
    "\n",
    "# Let’s look at the final overal results from interactively fitting PRIM to the data. \n",
    "# For this, transform the stats and boxes to pandas data frames.\n",
    "# tabluates both boxes for comparison\n",
    "print prim_alg.stats_to_dataframe()\n",
    "print prim_alg.boxes_to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CART\n",
    "from analysis import cart\n",
    "cart_alg = cart.CART(x,y, 0.05)\n",
    "cart_alg.build_tree()\n",
    "\n",
    "# can transform to df directly\n",
    "print cart_alg.stats_to_dataframe()\n",
    "print cart_alg.boxes_to_dataframe()\n",
    "\n",
    "# use show_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
=======
    "# SOBOL viz\n",
    "# SOBOL indices bar plot\n",
    "sns.set_style('white')\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "indices = sobol_df[['S1','ST']]\n",
    "err = sobol_df[['S1_conf','ST_conf']]\n",
    "\n",
    "indices.plot.bar(yerr=err.values.T,ax=ax)\n",
    "fig.set_size_inches(15,8)\n",
    "fig.subplots_adjust(bottom=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> 494596a53c200ee8f66c578fe9701c496a9ec4dd
    "## Scenario discovery methods\n",
    "\n",
    "# method 1: proper Scenario Discovery. PRIM, CART, \n",
    "\n",
    "# Method 2: extra tress feature scoring\n",
    "# a poor man’s alternative to global sensitivity analysis\n",
    "\n",
    "cleaned_experiments_sobol = experiments_sobol.drop(labels=[l.name for l in dike_model.uncertainties], axis=1)\n",
    "\n",
    "scores,_ = feature_scoring.get_ex_feature_scores(cleaned_experiments_sobol, y_damage, RuleInductionType.REGRESSION, #nr_trees=100,\n",
    "                                      max_features=0.6)\n",
    "scores\n",
    "\n",
    "# method 3. Dimensional Stacking: visual, building on feature scroing\n",
    "# two steps: identifying the most important uncertainties that affect system behavior, and creating a pivot table using the most influential uncertainties. \n",
    "# In order to do this, we first need, as in scenario discovery, specify the outcomes that are of interest. The creating of the pivot table involves binning the uncertainties\n",
    "\n",
    "cleaned_experiments = experiments.drop(labels=[l.name for l in dike_model.levers], axis=1)\n",
    "\n",
    "from ema_workbench.analysis import dimensional_stacking\n",
    "dimensional_stacking.create_pivot_plot(cleaned_experiments, y)\n",
    "plt.show()\n",
    "\n",
    "# mentod 4. regional sensitivity analysis.\n",
    "## Regional sensitivity analysis plot\n",
    "\n",
    "from ema_workbench.analysis import regional_sa\n",
    "from numpy.lib import recfunctions as rf\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "# model is the same across experiments\n",
    "x = experiments.copy()\n",
    "x = x.drop('model', axis=1)\n",
    "y = outcomes['Expected Annual Damage'] < 0.8\n",
    "fig = regional_sa.plot_cdfs(x,y)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nfe convergence\n",
    "\n",
    "ref_scenario = Scenario('reference', **scen1)\n",
    "\n",
    "espilon = [100000,100000,0.1,100000,100000] ### Epsilon values are based on the scales of the output parameters\n",
    "\n",
    "convergence_metrics = [HyperVolume(minimum=[0,0,0,0,0], maximum=[3e+08, 2e+08,0.4,2e+08,2e+08]),  ### Numbers selected with generated of earlier optimalisations\n",
    "                      EpsilonProgress()]\n",
    "\n",
    "nfe = 2500    ### 25.000                                                                    ### Witch 25.000 runs convergence is almost perfect\n",
    "\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "        results_opt, convergence = evaluator.optimize(nfe=nfe,\n",
    "                                                 searchover='levers',\n",
    "                                                 epsilons=espilon,\n",
    "                                                 convergence=convergence_metrics,\n",
    "                                                 reference=ref_scenario)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## directed search  methods ###########################\n",
    "# optimization strategies/ searches\n",
    "# 1. Search over the decision levers, conditional on a reference scenario = candidate strategies, step 1 in MORDM\n",
    "# 2. Search over the uncertain factors, conditional on a reference policy \n",
<<<<<<< HEAD
    "# 3. Search over the decision levers given a set of scenarios: e.g for robust optimization over scenarios"
=======
    "# 3. Search over the decision levers given a set of scenarios"
>>>>>>> 494596a53c200ee8f66c578fe9701c496a9ec4dd
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specicy scalar outcome direction (min/max/info)\n",
    "# keep nfe high, based on convergence considerations\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.optimize(nfe=250, searchover='levers',\n",
    "                                 epsilons=[0.1,]*len(model.outcomes))\n",
    "\n",
    "## Constrained inputs optimization problem\n",
    "\n",
    "# A constraint is essentially a function that should return the distance from the feasibility threshold. The distance should be 0 if the constraint is met.\n",
    "\n",
    "from ema_workbench import Constraint\n",
    "\n",
    "constraints = [Constraint(\"max pollution\", outcome_names=\"max_P\", function=lambda x:max(0, x-1))]\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.optimize(nfe=250, searchover='levers', epsilons=[0.1,]*len(model.outcomes),constraints=constraints)\n",
    "\n",
    "## Tracking convergence\n",
    "# mportant part of using many-objective evolutionary algorithms\n",
    "# ema has two methods: hypervolume & epsilon progress\n",
    "from ema_workbench.em_framework.optimization import (HyperVolume,\n",
    "                                                     EpsilonProgress)\n",
    "\n",
    "convergence_metrics = [HyperVolume(minimum=[0,0,0,0], maximum=[1,1.01,1.01,1.01]), EpsilonProgress()]\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results, convergence = evaluator.optimize(nfe=10000, searchover='levers', epsilons=[0.05,]*len(model.outcomes),convergence=convergence_metrics, constraints=constraints)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "ax1.set_ylabel('$\\epsilon$-progress')\n",
    "ax2.plot(convergence.nfe, convergence.hypervolume)\n",
    "ax2.set_ylabel('hypervolume')\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Controlling reference scenario\n",
    "\n",
    "with MultiprocessingEvaluator(lake_model) as evaluator:\n",
    "    results = evaluator.optimize(searchover='levers', nfe=1000,\n",
    "                       epsilons=[0.1, ]*len(lake_model.outcomes),\n",
    "                       reference=reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search over uncertainties: worst case discovery\n",
    "\n",
    "# change the kind (direction) on each outcome so that we search for the worst outcome, and searchover uncertainties\n",
    "# change outcomes so direction is undesirable\n",
    "minimize = ScalarOutcome.MINIMIZE\n",
    "maximize = ScalarOutcome.MAXIMIZE\n",
    "\n",
    "for outcome in model.outcomes:\n",
    "    if outcome.kind == minimize:\n",
    "        outcome.kind = maximize\n",
    "    else:\n",
    "        outcome.kind = minimize\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.optimize(nfe=1000, searchover='uncertainties',\n",
    "                                 epsilons=[0.1,]*len(model.outcomes))\n",
    "\n",
    "## Robust search\n",
    "\n",
    "\n",
    "# optimize levers/ uncertainties over a set of policies/scenarios instead of a single reference one\n",
    "# Step 1: Choose robustness metric\n",
    "# robustness metric takes as input the performance of a candidate policy over a set of scenarios and returns a single robustness score. The function takes numpy array input and gives single number output. e.g percentile\n",
    "\n",
    "mport functools\n",
    "\n",
    "percentile10 = functools.partial(np.percentile, q=10)\n",
    "percentile90 = functools.partial(np.percentile, q=90)\n",
    "\n",
    "MAXIMIZE = ScalarOutcome.MAXIMIZE\n",
    "MINIMIZE = ScalarOutcome.MINIMIZE\n",
    "robustnes_functions = [ScalarOutcome('90th percentile max_p', kind=MINIMIZE,\n",
    "                             variable_name='max_P', function=percentile90),\n",
    "                       ScalarOutcome('10th percentile reliability', kind=MAXIMIZE,\n",
    "                             variable_name='reliability', function=percentile10),\n",
    "                       ScalarOutcome('10th percentile inertia', kind=MAXIMIZE,\n",
    "                             variable_name='inertia', function=percentile10),\n",
    "                       ScalarOutcome('10th percentile utility', kind=MAXIMIZE,\n",
    "                             variable_name='utility', function=percentile10)]\n",
    "\n",
    "# Step 2: generate scenarios\n",
    "from ema_workbench.em_framework import sample_uncertainties\n",
    "\n",
    "n_scenarios = 50\n",
    "scenarios = sample_uncertainties(model, n_scenarios)\n",
    "\n",
    "# Step 3: robust optimization\n",
    "nfe = int(1e6)\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    robust_results = evaluator.robust_optimize(robustnes_functions, scenarios,\n",
    "                            nfe=nfe, epsilons=[0.05,]*len(robustnes_functions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parallel coordinate plots viz (paracoords)\n",
    "from ema_workbench.analysis import parcoords\n",
    "data = results_opt.loc[:, [o.name for o in dike_model.outcomes]]\n",
    "limits = parcoords.get_limits(data)\n",
    "print(limits)\n",
    "limits.loc[0, ['Expected Annual Damage', 'Dike Investment Costs' , 'Expected Number of Deaths','Evacuation Costs','RfR Investment Costs' ]] = 0\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.4. Evaluating canidate solutions under uncertainty\n",
    "\n",
    "### Which policies options are in the canidate set?\n",
    "policies = results_opt\n",
    "policies = policies.drop([o.name for o in dike_model.outcomes], axis=1)\n",
    "#policies\n",
    "\n",
    "### Save the policies\n",
    "policies.to_excel('policies.xls')\n",
    "\n",
    "### Makes a distinction between RfR policies and DH only policies\n",
    "policies['RfR_SUM'] = policies.loc[:,'0_RfR 0':'4_RfR 2'].sum(axis = 1)\n",
    "RfR = policies[policies['RfR_SUM'] >= 1]\n",
    "DH = policies[policies['RfR_SUM'] == 0]\n",
    "\n",
    "### We are only interested in policies  with less than 0.01 expected deaths (1 expected death every 100 year). \n",
    "logical = results_opt['Expected Number of Deaths'] < 0.01\n",
    "np.sum(logical) ### How many policy options are still valid?\"\n",
    "\n",
    "### Show RfR policies to evaluate\n",
    "RfR[logical]\n",
    "\n",
    "### Policies to evaluate (Both DH and RfR)\n",
    "policies_exp = policies[logical]\n",
    "\n",
    "#policies_exp\n",
    "\n",
    "policies_exp.to_excel('policies_to_evaluate.xls')\n",
    "\n",
    "policies_to_evaluate = []\n",
    "\n",
    "for i, policy in policies_exp.iterrows():\n",
    "    policies_to_evaluate.append(Policy(str(i), **policy.to_dict()))\n",
    "\n",
    "### 3.6. Running the policies under uncertainty\"\n",
    "\n",
    "# Choose problem formuation\n",
    "dike_model, planning_steps = get_model_for_problem_formulation(2)\n",
    "\n",
    "### Selecting number of scenarios to run the policies with\n",
    "n_scenarios = 2000 ### 2000\n",
    "\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    results_robust = evaluator.perform_experiments(n_scenarios,\n",
    "                                           policies_to_evaluate)"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "pydice_folder = os.path.dirname(os.getcwd())+\"\\\\1_Model\"\n",
    "policy_folder = os.path.dirname(os.getcwd())+\"\\\\5_Policy_Discovery\"\n",
    "sys.path.insert(1, pydice_folder)\n"
   ]
=======
>>>>>>> 494596a53c200ee8f66c578fe9701c496a9ec4dd
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}