{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'e:\\\\Year_2_Quarter_4\\\\Thesis\\\\06_Code'"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir(os.getcwd())\n",
    "pydice_folder = os.path.dirname(os.getcwd()) + \"\\\\06_Code\"\n",
    "sys.path.insert(1, pydice_folder)\n",
    "pydice_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introdction to the code\n",
    "\n",
    "### Importing all the nessesarry libraries\n",
    " \n",
    "\n",
    "from __future__ import (unicode_literals, print_function, absolute_import,\n",
    "                        division)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "C:\\Users\\shrid\\Anaconda3\\lib\\site-packages\\ema_workbench\\analysis\\prim.py:31: ImportWarning: altair based interactive inspection not available\n  \"inspection not available\"), ImportWarning)\nC:\\Users\\shrid\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n  return f(*args, **kwds)\n"
    }
   ],
   "source": [
    "#Importing the needed Ema workbench libraries\n",
    "\n",
    "from ema_workbench import (Model, Policy, CategoricalParameter,ScalarOutcome, IntegerParameter, RealParameter, optimize, Scenario)\n",
    "\n",
    "from ema_workbench import ema_logging, MultiprocessingEvaluator, SequentialEvaluator\n",
    "\n",
    "from ema_workbench.em_framework.optimization import EpsilonProgress, HyperVolume\n",
    "\n",
    "from ema_workbench import (MultiprocessingEvaluator, ema_logging, perform_experiments, Constant)\n",
    "\n",
    "from ema_workbench.em_framework.salib_samplers import get_SALib_problem\n",
    "\n",
    "from ema_workbench.em_framework.samplers import sample_levers, sample_uncertainties\n",
    "\n",
    "from ema_workbench.em_framework.evaluators import LHS, SOBOL, MORRIS, SequentialEvaluator, BaseEvaluator\n",
    "\n",
    "from SALib.analyze import sobol\n",
    "\n",
    "from ema_workbench.analysis import parcoords\n",
    "\n",
    "from ema_workbench.util import ema_logging\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "from ema_workbench import save_results, load_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from MyDICE_v2 import PyDICE\n",
    "dice_sm = Model('EMA', function= Model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify inputs\n",
    "dice_sm.uncertainties = [ IntegerParameter('t2xco2_index', 0, 999),\n",
    "                            IntegerParameter('t2xco2_dist',0 , 2),\n",
    "                            # IntegerParameter('fdamage', 0, 2),\n",
    "                            RealParameter('tfp_gr', 0.07, 0.09),\n",
    "                            RealParameter('sigma_gr', -0.012, -0.008),\n",
    "                            # RealParameter('emdd', 1.0, 2.0)\n",
    "                            RealParameter('pop_gr', 0.1, 0.15),\n",
    "                            RealParameter('fosslim',  4000.0, 13649),\n",
    "                            IntegerParameter('cback', 100, 600)]\n",
    "\n",
    "dice_sm.levers = [RealParameter('sr', 0.1, 0.5),\n",
    "                      RealParameter('prtp_con',  0.001, 0.015),\n",
    "                      RealParameter('prtp_dam',  0.001, 0.015),\n",
    "                    #   RealParameter('emuc', 1.0, 2.0),\n",
    "                      \n",
    "                      IntegerParameter('periodfullpart', 10, 58),\n",
    "                      IntegerParameter('miu_period', 10, 58)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_sm.outcomes = [ScalarOutcome('Damages', ScalarOutcome.INFO),\n",
    "                        ScalarOutcome('Disutility of Damage', ScalarOutcome.INFO),\n",
    "                        ScalarOutcome('Total Output', ScalarOutcome.INFO),\n",
    "                        ScalarOutcome('Welfare', ScalarOutcome.INFO)\n",
    "                        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# 1. Open Exploration\"\n",
    "\n",
    "#Choose problem formuation\n",
    "# model, planning_steps = get_model_for_problem_formulation(EMA) ### 2\"\n",
    "\n",
    "### 1.1. Running model with random scenarios and policies\n",
    "\n",
    "#We start with running the model with 10 random generated policies with 250 scenarios each. By doing this the behavior of the model can be examineted. \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[MainProcess/INFO] pool started\n[MainProcess/INFO] performing 10 scenarios * 10 policies * 1 model(s) = 100 experiments\n"
    }
   ],
   "source": [
    "with MultiprocessingEvaluator(dice_sm, n_processes= 8) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios=10, policies=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "\n",
    "\n",
    "from ema_workbench.analysis import pairs_plotting\n",
    "\n",
    "fig, axes = pairs_plotting.pairs_scatter(experiments_open, outcomes_open, group_by='policy',\n",
    "                                         legend=True)\n",
    "fig.set_size_inches(20,20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  ### 1.2. PRIM\n",
    "    \n",
    "  #PRIM is used to see which variables are influencing the results the most. By selecting interesting cases out off all cases by giving one or more restrains. We do this by selecting all cases with \\\"Expected Annual Damage\\\" is lower than 0.4 and later on also by selecting all cases with Expected number of deaths lower than 0.01.\"\n",
    "from ema_workbench.analysis import prim\n",
    "from ema_workbench.analysis import scenario_discovery_util as sdutil\n",
    "from sklearn import preprocessing \n",
    "\n",
    "x = experiments_open\n",
    "y = outcomes_open['Expected Annual Damage'] < 0.4\n",
    "prim_alg = prim.Prim(x, y, threshold=0.8, peel_alpha = 0.1)\n",
    "box1 = prim_alg.find_box()\n",
    "\n",
    "box1.show_tradeoff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.amax(outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box1.show_tradeoff()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box1.inspect(4)\n",
    "#box1.inspect(4, style='graph')\n",
    "#plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box1.select(5)\n",
    "#fig = box1.show_pairs_scatter(4)\n",
    "#fig.set_size_inches((12,12))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = experiments_open\n",
    "y = outcomes_open['Expected Number of Deaths'] < 0.005\n",
    "prim_alg = prim.Prim(x, y, threshold=0.8, peel_alpha = 0.1)\n",
    "box2 = prim_alg.find_box()\n",
    "\n",
    "box2.show_tradeoff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box2.inspect(5)\n",
    "#box2.inspect(5, style='graph')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box1.select(5)\n",
    "#fig = box2.show_pairs_scatter(5)\n",
    "#fig.set_size_inches((12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Base Case\n",
    "\n",
    "#To see which uncertaincies have the most influence in the current situation the base case is evaluated. This has been done by setting all policy levers at 0 (No measure taken). Then the model has been runned with a random sampled uncertaincy space (Sobol is used for doing so). After that the influence of the uncairtainties on the indicators are visualised.\n",
    "\n",
    "# checking policy levers\n",
    "list(model.levers)\n",
    "\n",
    "# Initialization for SALib\n",
    "uncertainty_problem = get_SALib_problem(model.uncertainties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1 Defining Zero Policy Scenarios\n",
    "\n",
    "#To define the zero plicy scenario we put all policy levers on 0. By doing this we can evaluate what will happens when we are not doing anything in the upcoming years. This will show us if doing nothing is an option or that real action is needed to prevent floods on the IJssel river.\n",
    "\n",
    "# Choose problem formuation\n",
    "model, planning_steps = get_model_for_problem_formulation(2)\n",
    "\n",
    "zero = {'DikeIncrease 0': 0, 'DikeIncrease 1': 0, 'DikeIncrease 2': 0, 'RfR 0': 0, 'RfR 1': 0, 'RfR 2': 0}\n",
    "zero_pol = {}\n",
    "\n",
    "for key in model.levers:\n",
    "    s1, s2 = key.name.split('_')\n",
    "    zero_pol.update({key.name: zero[s2]})\n",
    "zero_pol = Policy('Policy 0', **zero_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of uncertainties in the analysis\n",
    "n_uncertainties_sobol = 10 ### 100\"\n",
    "\n",
    "#Running the analysis \n",
    "with SequentialEvaluator(model) as evaluator:\n",
    "    results_sobol = evaluator.perform_experiments(n_uncertainties_sobol, zero_pol, uncertainty_sampling=SOBOL, levers_sampling=SOBOL)\n",
    "\n",
    "# Splitting the experiments from the outcomes of the result\n",
    "experiments_sobol, outcomes_sobol = results_sobol\n",
    "\n",
    "# Show which outcomes are measured \n",
    "print(outcomes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2. Seperating Outcomes of Interest\n",
    "\n",
    "#Disaggregated for location (3)\n",
    "A1_Total_Costs = outcomes_sobol['A.1 Total Costs']\n",
    "A1_Expected_Number_of_Deaths = outcomes_sobol['A.1_Expected Number of Deaths']\n",
    "A2_Total_Costs = outcomes_sobol['A.2 Total Costs']\n",
    "A2_Expected_Number_of_Deaths= outcomes_sobol['A.2_Expected Number of Deaths']\n",
    "A3_Total_Costs = outcomes_sobol['A.3 Total Costs']\n",
    "A3_Expected_Number_of_Deaths= outcomes_sobol['A.3_Expected Number of Deaths']\n",
    "A4_Total_Costs = outcomes_sobol['A.4 Total Costs']\n",
    "A4_Expected_Number_of_Deaths= outcomes_sobol['A.4_Expected Number of Deaths']\n",
    "A5_Total_Costs = outcomes_sobol['A.5 Total Costs']\n",
    "A5_Expected_Number_of_Deaths= outcomes_sobol['A.5_Expected Number of Deaths']\n",
    "\n",
    "#Aggregated (2)\n",
    "Expected_Annual_Damage = outcomes_sobol['Expected Annual Damage']\n",
    "Expected_Number_of_Deaths = outcomes_sobol['Expected Number of Deaths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the outcomes to Z-scores by using the sobol analze option\n",
    "A1_Total_Costs_Score = sobol.analyze(uncertainty_problem, A1_Total_Costs, calc_second_order=True, print_to_console=False)\n",
    "A1_Expected_Number_of_Deaths_Score = sobol.analyze(uncertainty_problem, A1_Expected_Number_of_Deaths, calc_second_order=True, print_to_console=False)\n",
    "\n",
    "A2_Total_Costs_Score = sobol.analyze(uncertainty_problem, A2_Total_Costs, calc_second_order=True, print_to_console=False)\n",
    "A2_Expected_Number_of_Deaths_Score = sobol.analyze(uncertainty_problem, A2_Expected_Number_of_Deaths, calc_second_order=True, print_to_console=False)\n",
    "\n",
    "A3_Total_Costs_Score = sobol.analyze(uncertainty_problem, A3_Total_Costs, calc_second_order=True, print_to_console=False)\n",
    "A3_Expected_Number_of_Deaths_Score = sobol.analyze(uncertainty_problem, A3_Expected_Number_of_Deaths, calc_second_order=True, print_to_console=False)\n",
    "\n",
    "A4_Total_Costs_Score = sobol.analyze(uncertainty_problem, A4_Total_Costs, calc_second_order=True, print_to_console=False)\n",
    "A4_Expected_Number_of_Deaths_Score = sobol.analyze(uncertainty_problem, A4_Expected_Number_of_Deaths, calc_second_order=True, print_to_console=False)\n",
    "\n",
    "A5_Total_Costs_Score = sobol.analyze(uncertainty_problem, A5_Total_Costs, calc_second_order=True, print_to_console=False)\n",
    "A5_Expected_Number_of_Deaths_Score = sobol.analyze(uncertainty_problem, A5_Expected_Number_of_Deaths, calc_second_order=True, print_to_console=False)\n",
    "\n",
    "Expected_Annual_Damage_Score = sobol.analyze(uncertainty_problem, Expected_Annual_Damage, calc_second_order=True, print_to_console=False)\n",
    "Expected_Number_of_Deaths_Score = sobol.analyze(uncertainty_problem, Expected_Number_of_Deaths, calc_second_order=True, print_to_console=False)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.3 Filtering the Scores\"\n",
    "\n",
    "A1_Total_Costs_Score_filtered = {k:A1_Total_Costs_Score[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "A1_Expected_Number_of_Deaths_Score_filtered = {k:A1_Expected_Number_of_Deaths_Score[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "\n",
    "A2_Total_Costs_Score_filtered = {k:A2_Total_Costs_Score[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "A2_Expected_Number_of_Deaths_Score_filtered = {k:A2_Expected_Number_of_Deaths_Score[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "\n",
    "A3_Total_Costs_Score_filtered = {k:A3_Total_Costs_Score[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "A3_Expected_Number_of_Deaths_Score_filtered = {k:A3_Expected_Number_of_Deaths_Score[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "\n",
    "A4_Total_Costs_Score_filtered = {k:A4_Total_Costs_Score[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "A4_Expected_Number_of_Deaths_Score_filtered = {k:A4_Expected_Number_of_Deaths_Score[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "\n",
    "A5_Total_Costs_Score_filtered = {k:A5_Total_Costs_Score[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "A5_Expected_Number_of_Deaths_Score_filtered = {k:A5_Expected_Number_of_Deaths_Score[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "\n",
    "Expected_Annual_Damage_Score_filtered = {k:Expected_Annual_Damage_Score[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "Expected_Number_of_Deaths_Score_filtered = {k:Expected_Number_of_Deaths_Score[k] for k in ['ST','ST_conf','S1','S1_conf']}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.4. Creating data frame\"\n",
    "\n",
    "A1_Total_Costs_Score_df = pd.DataFrame(A1_Total_Costs_Score_filtered, index=uncertainty_problem['names'])\n",
    "A1_Expected_Number_of_Deaths_Score_df = pd.DataFrame(A1_Expected_Number_of_Deaths_Score_filtered, index=uncertainty_problem['names'])\n",
    "\n",
    "A2_Total_Costs_Score_df = pd.DataFrame(A2_Total_Costs_Score_filtered, index=uncertainty_problem['names'])\n",
    "A2_Expected_Number_of_Deaths_Score_df = pd.DataFrame(A2_Expected_Number_of_Deaths_Score_filtered, index=uncertainty_problem['names'])\n",
    "\n",
    "A3_Total_Costs_Score_df = pd.DataFrame(A3_Total_Costs_Score_filtered, index=uncertainty_problem['names'])\n",
    "A3_Expected_Number_of_Deaths_Score_df = pd.DataFrame(A3_Expected_Number_of_Deaths_Score_filtered, index=uncertainty_problem['names'])\n",
    "\n",
    "A4_Total_Costs_Score_df = pd.DataFrame(A4_Total_Costs_Score_filtered, index=uncertainty_problem['names'])\n",
    "A4_Expected_Number_of_Deaths_Score_df = pd.DataFrame(A4_Expected_Number_of_Deaths_Score_filtered, index=uncertainty_problem['names'])\n",
    "\n",
    "A5_Total_Costs_Score_df = pd.DataFrame(A5_Total_Costs_Score_filtered, index=uncertainty_problem['names'])\n",
    "A5_Expected_Number_of_Deaths_Score_df = pd.DataFrame(A5_Expected_Number_of_Deaths_Score_filtered, index=uncertainty_problem['names'])\n",
    "\n",
    "Expected_Annual_Damage_Score_df = pd.DataFrame(Expected_Annual_Damage_Score_filtered, index=uncertainty_problem['names'])\n",
    "Expected_Number_of_Deaths_Score_df = pd.DataFrame(Expected_Number_of_Deaths_Score_filtered, index=uncertainty_problem['names'])\"\n",
    "\n",
    "df_Sobol = pd.DataFrame(data=None, index=Expected_Number_of_Deaths_Score_df.index, columns=['A.1 Total Costs', 'A.1_Expected Number of Deaths', 'A.2 Total Costs', 'A.2_Expected Number of Deaths', 'A.3 Total Costs', 'A.3_Expected Number of Deaths', 'A.4 Total Costs', 'A.4_Expected Number of Deaths', 'A.5 Total Costs', 'A.5_Expected Number of Deaths', 'Expected Annual Damage', 'Expected Number of Deaths'])\"\n",
    "\n",
    "df_Sobol['A.1 Total Costs'] = A1_Total_Costs_Score_df\n",
    "df_Sobol['A.1_Expected Number of Deaths'] = A1_Expected_Number_of_Deaths_Score_df\n",
    "\n",
    "df_Sobol['A.2 Total Costs'] = A2_Total_Costs_Score_df\n",
    "df_Sobol['A.2_Expected Number of Deaths'] = A2_Expected_Number_of_Deaths_Score_df\n",
    "\n",
    "df_Sobol['A.3 Total Costs'] = A3_Total_Costs_Score_df\n",
    "df_Sobol['A.3_Expected Number of Deaths'] = A3_Expected_Number_of_Deaths_Score_df\n",
    "\n",
    "df_Sobol['A.4 Total Costs'] = A4_Total_Costs_Score_df\n",
    "df_Sobol['A.4_Expected Number of Deaths'] = A4_Expected_Number_of_Deaths_Score_df\n",
    "\n",
    "df_Sobol['A.5 Total Costs'] = A5_Total_Costs_Score_df\n",
    "df_Sobol['A.5_Expected Number of Deaths'] = A5_Expected_Number_of_Deaths_Score_df\n",
    "\n",
    "df_Sobol['Expected Annual Damage'] = Expected_Annual_Damage_Score_df[['ST']]\n",
    "df_Sobol['Expected Number of Deaths'] = Expected_Number_of_Deaths_Score_df[['ST']]\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.5 Creating a visualisation of the results\"\n",
    "\n",
    "ax = sns.heatmap(df_Sobol.T, cmap='viridis', annot=True)\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(30,9)\n",
    "\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 3. Optimalisation\"\n",
    "\n",
    "### 3.1. Searching for candidate solutions with MORO\n",
    "\n",
    "#To come up with a good policy first we need to see which possible solution are in a costefficient way improving the safety (lower expected yearly damage and yearly deaths because of floodings). The investment costs should ofcourse also be as low as possible.\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "   dike_model, planning_steps = get_model_for_problem_formulation(1)    \n",
    "\n",
    "   reference_values = {'Bmax': 175, 'Brate': 1.5, 'pfail': 0.5,\n",
    "                       'discount rate': 3.5,\n",
    "                       'ID flood wave shape': 4}\n",
    "   scen1 = {}\n",
    "\n",
    "\n",
    "   #### Credits for solving namesplit problem and sharing this fix with us to group \\\"Vera\\\n",
    "   for key in dike_model.uncertainties:\n",
    "       name_split = key.name.split('_')\n",
    "       name_split2 = ''.join([i for i in key.name if not i.isdigit()]).rstrip().split('_')\n",
    "\n",
    "       if len(name_split) == 1:\n",
    "           scen1.update({key.name: reference_values[name_split2[0]]})\n",
    "\n",
    "       else:\n",
    "           scen1.update({key.name: reference_values[name_split[1]]})\n",
    "\n",
    "   ref_scenario = Scenario('reference', **scen1)\n",
    "\n",
    "   espilon = [100000,100000,0.1] ### Epsilon values are based on the scales of the output parameters\n",
    "\n",
    "   convergence_metrics = [HyperVolume(minimum=[0,0,0], maximum=[3e+08, 2.5e+08,0.4]),  ### Numbers selected with generated of earlier optimalisations\n",
    "                      EpsilonProgress()]\n",
    "\n",
    "   nfe = 25000    ### 25.000                                                                    ### Witch 25.000 runs convergence is almost perfect\n",
    "\n",
    "   with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "       results_opt, convergence = evaluator.optimize(nfe=nfe,\n",
    "                                                 searchover='levers',\n",
    "                                                 epsilons=espilon,\n",
    "                                                 convergence=convergence_metrics,\n",
    "                                                 reference=ref_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2 Visualisation of the MORO convergence KPI's\"\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "ax1.set_ylabel('$\\\\epsilon$-progress')\n",
    "ax2.plot(convergence.nfe, convergence.hypervolume)\n",
    "ax2.set_ylabel('hypervolume')\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.3. Visualisation of the performance of the canidate solutions on the KPI's\"\n",
    "\n",
    "data = results_opt.loc[:, [o.name for o in dike_model.outcomes]]\n",
    "limits = parcoords.get_limits(data)\n",
    "print(limits)\n",
    "limits.loc[0, ['Expected Annual Damage', 'Total Investment Costs', 'Expected Number of Deaths']] = 0\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "\n",
    "### Because lower scores are better we change the axis for all parameters\n",
    "paraxes.invert_axis(['Expected Annual Damage', 'Total Investment Costs' , 'Expected Number of Deaths'])\n",
    "plt.show()\n",
    "\n",
    "outcomes_opt = results_opt.loc[:, ['Expected Annual Damage', 'Total Investment Costs', 'Expected Number of Deaths']]\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(outcomes_opt['Expected Annual Damage'], outcomes_opt['Total Investment Costs'], outcomes_opt['Expected Number of Deaths'])\n",
    "ax.set_xlabel('Expected Annual Damage')\n",
    "ax.set_ylabel('Total Investment Costs')\n",
    "ax.set_zlabel('Expected Number of Deaths')\n",
    "plt.show()\n",
    "\n",
    "### 3.4. Evaluating canidate solutions under uncertainty\n",
    "\n",
    "### Which policies options are in the canidate set?\n",
    "policies = results_opt\n",
    "policies = policies.drop([o.name for o in dike_model.outcomes], axis=1)\n",
    "#policies\n",
    "\n",
    "### Save the policies\n",
    "policies.to_excel('policies.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.5. Making a selection of policies to evaluate under uncertainty\"\n",
    "\n",
    "### Makes a distinction between RfR policies and DH only policies\n",
    "policies['RfR_SUM'] = policies.loc[:,'0_RfR 0':'4_RfR 2'].sum(axis = 1)\n",
    "RfR = policies[policies['RfR_SUM'] >= 1]\n",
    "DH = policies[policies['RfR_SUM'] == 0]\"\n",
    "\n",
    "### We are only interested in policies  with less than 0.01 expected deaths (1 expected death every 100 year). \n",
    "logical = results_opt['Expected Number of Deaths'] < 0.01\n",
    "np.sum(logical) ### How many policy options are still valid?\"\n",
    "\n",
    "### Show RfR policies to evaluate\n",
    "RfR[logical]\n",
    "\n",
    "### Show DH policies to evaluate\n",
    "DH[logical]\n",
    "\n",
    "### Policies to evaluate (Both DH and RfR)\n",
    "policies_exp = policies[logical]\n",
    "\n",
    "#policies_exp\n",
    "\n",
    "policies_exp.to_excel('policies_to_evaluate.xls')\n",
    "\n",
    "policies_to_evaluate = []\n",
    "\n",
    "for i, policy in policies_exp.iterrows():\n",
    "    policies_to_evaluate.append(Policy(str(i), **policy.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.6. Running the policies under uncertainty\"\n",
    "\n",
    "# Choose problem formuation\n",
    "dike_model, planning_steps = get_model_for_problem_formulation(2)\"\n",
    "\n",
    "### Selecting number of scenarios to run the policies with\n",
    "n_scenarios = 2000 ### 2000\n",
    "\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    results_robust = evaluator.perform_experiments(n_scenarios,\n",
    "                                           policies_to_evaluate)\"\n",
    "\n",
    "### saving the results\n",
    "save_results(results_robust, 'MORDM_reevaluation.tar.gz')\"\n",
    "\n",
    "results_robust = load_results('MORDM_reevaluation.tar.gz')\n",
    "\n",
    "experiments_robust, outcomes_robust = results_robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.7. Visualising the results\"\n",
    "\n",
    "from ema_workbench.analysis import pairs_plotting\n",
    "\n",
    "fig, axes = pairs_plotting.pairs_scatter(experiments_robust, outcomes_robust, group_by='policy',\n",
    "                                        legend=True)\n",
    "fig.set_size_inches(20,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.8 How robust are the policies (Calculating regret)\n",
    "\n",
    "def s_to_n(data, direction):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "\n",
    "    if direction==ScalarOutcome.MAXIMIZE:\n",
    "        return mean/std\n",
    "    else:\n",
    "        return mean*std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments, outcomes = results_robust\n",
    "\n",
    "overall_scores = {}\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    scores = {}\n",
    "\n",
    "    logical = experiments['policy']==policy\n",
    "\n",
    "    for outcome in dike_model.outcomes:\n",
    "        value  = outcomes[outcome.name][logical]\n",
    "        sn_ratio = s_to_n(value, outcome.kind)\n",
    "        scores[outcome.name] = sn_ratio\n",
    "    overall_scores[policy] = scores\n",
    "scores = pd.DataFrame.from_dict(overall_scores).T\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regret(data, best):\n",
    "   return np.abs(best-data)\"\n",
    "\n",
    "overall_regret = {}\n",
    "max_regret = {}\n",
    "for outcome in dike_model.outcomes:\n",
    "   policy_column = experiments['policy']\n",
    "\n",
    "   # create a DataFrame with all the relevent information\n",
    "   # i.e., policy, scenario_id, and scores\n",
    "   data = pd.DataFrame({outcome.name: outcomes[outcome.name], \n",
    "                        \\\"policy\\\":experiments['policy'],\n",
    "                        \\\"scenario\\\":experiments['scenario']})\n",
    "\n",
    "   # reorient the data by indexing with policy and scenario id\n",
    "   data = data.pivot(index='scenario', columns='policy')\n",
    "\n",
    "   # flatten the resulting hierarchical index resulting from \n",
    "   # pivoting, (might be a nicer solution possible)\n",
    "   data.columns = data.columns.get_level_values(1)\n",
    "\n",
    "   # we need to control the broadcasting. \n",
    "   # max returns a 1d vector across scenario id. By passing\n",
    "   # np.newaxis we ensure that the shape is the same as the data\n",
    "   # next we take the absolute value\n",
    "   #\n",
    "   # basically we take the difference of the maximum across \n",
    "   # the row and the actual values in the row\n",
    "   #\n",
    "   outcome_regret = (data.max(axis=1)[:, np.newaxis] - data).abs()\n",
    "\n",
    "   overall_regret[outcome.name] = outcome_regret\n",
    "   max_regret[outcome.name] = outcome_regret.max()\n",
    "\n",
    "\n",
    "max_regret = pd.DataFrame(max_regret)\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.heatmap(max_regret/max_regret.max(), cmap='viridis', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.9. Visualising the data with Paracoords\"\n",
    "\n",
    "data = scores\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['Expected Annual Damage', 'Dike Investment Costs', 'RfR Investment Costs', 'Evacuation Costs','Expected Number of Deaths']] = 0\n",
    "#print(limits)\n",
    "paraxes = parcoords.ParallelAxes(limits, fontsize = 12, rot = 90)\n",
    "#paraxes.legend()\n",
    "paraxes.plot(data)\n",
    "plt.show()\n",
    "\n",
    "### 3.10. Scenario discovery with PRIM\"\n",
    "\n",
    "x = experiments\n",
    "y = outcomes['Expected Annual Damage'] < 5e7\n",
    "prim_alg = prim.Prim(x, y, threshold=0.8, peel_alpha = 0.1)\n",
    "box3 = prim_alg.find_box(\n",
    "\n",
    "box3.show_tradeoff()\n",
    "plt.show()\"\n",
    "\n",
    "\n",
    "box3.inspect(10)\n",
    "box3.inspect(10, style='graph')\n",
    "plt.show()\"\n",
    "\n",
    "#box1.select(5)\n",
    "fig = box3.show_pairs_scatter(10)\n",
    "#fig.set_size_inches((12,12))\n",
    "plt.show()\n",
    "\n",
    "x = experiments\n",
    "y = outcomes['Expected Number of Deaths'] < 0.005\n",
    "prim_alg = prim.Prim(x, y, threshold=0.8, peel_alpha = 0.1)\n",
    "box4 = prim_alg.find_box()\n",
    "\n",
    "\n",
    "box4.show_tradeoff()\n",
    "plt.show()\"\n",
    "\n",
    "box4.inspect(11)\n",
    "box4.inspect(11, style='graph')\n",
    "plt.show()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = box4.show_pairs_scatter(11)\n",
    "#fig.set_size_inches((12,12))\n",
    "plt.show()\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}